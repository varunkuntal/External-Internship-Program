{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assign # 2 - LSTM.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt_IG0krESrv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://gist.githubusercontent.com/phillipj/4944029/raw/75ba2243dd5ec2875f629bf5d79f6c1e4b5a8b46/alice_in_wonderland.txt > /dev/null 2>&1\n",
        "!mv alice_in_wonderland.txt wonderland.txt > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72WbNA_HvmvQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.optimizers import Adam\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4Rxv17ovuZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load ascii text and covert to lowercase\n",
        "filename = \"wonderland.txt\"\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()\n",
        "\n",
        "# Split text at full stop\n",
        "sentence_list = raw_text.split('\\n')\n",
        "\n",
        "raw_text = re.sub(r'[^\\w\\s]','',raw_text)\n",
        "raw_text = ' '.join(raw_text.split())\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# Remove all punctuations\n",
        "for i in range(0, len(sentence_list)):\n",
        "    sentence_list[i] = re.sub(r'[^\\w\\s]','',sentence_list[i])\n",
        "    sentence_list[i] = ' '.join(sentence_list[i].split())\n",
        "    \n",
        "# Empty array to store int form of sentence ints\n",
        "sentence_ints = [0] * len(sentence_list)\n",
        "for i in range(0, len(sentence_list)):\n",
        "    sentence_ints[i] = [0] * len(sentence_list[i])\n",
        "    \n",
        "for i in range(0, len(sentence_list)):\n",
        "    for j in range(0, len(sentence_list[i])):\n",
        "        sentence_ints[i][j] = char_to_int[sentence_list[i][j]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7gem795wcbk",
        "colab_type": "code",
        "outputId": "cadbbfa5-21bd-478e-cc23-3d6d62a57ea1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "char_to_int"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{' ': 0,\n",
              " '0': 1,\n",
              " '3': 2,\n",
              " '_': 3,\n",
              " 'a': 4,\n",
              " 'b': 5,\n",
              " 'c': 6,\n",
              " 'd': 7,\n",
              " 'e': 8,\n",
              " 'f': 9,\n",
              " 'g': 10,\n",
              " 'h': 11,\n",
              " 'i': 12,\n",
              " 'j': 13,\n",
              " 'k': 14,\n",
              " 'l': 15,\n",
              " 'm': 16,\n",
              " 'n': 17,\n",
              " 'o': 18,\n",
              " 'p': 19,\n",
              " 'q': 20,\n",
              " 'r': 21,\n",
              " 's': 22,\n",
              " 't': 23,\n",
              " 'u': 24,\n",
              " 'v': 25,\n",
              " 'w': 26,\n",
              " 'x': 27,\n",
              " 'y': 28,\n",
              " 'z': 29}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da5mShWXjLOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "padded = pad_sequences(sequences=sentence_ints, maxlen=35, dtype='int', padding='post', value=0)\n",
        "# padded = np.array(sentence_ints)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VAXJZMz6WTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_ints = [x for x in sentence_ints if x != []]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqHuu5ETvuhA",
        "colab_type": "code",
        "outputId": "f99ee2f7-da40-431a-c151-87810226b20b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  134159\n",
            "Total Vocab:  30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eMsjEvyvujT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 35\n",
        "dataX = padded\n",
        "dataX = dataX[:-1]\n",
        "dataY = []\n",
        "for i in range(0, len(padded) - 1):\n",
        "# \tseq_in = raw_text[i:i + seq_length]\n",
        "# \tseq_out = raw_text[i + seq_length]\n",
        "#   dataX.append([char_to_int[char] for char in seq_in])\n",
        "    print(i)\n",
        "    dataY.append(padded[i+1][0])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UtIX5N7vuln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9FtNygnvuoc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "# # define the LSTM model\n",
        "# model = Sequential()\n",
        "# # model.add(Dropout(0.1))\n",
        "# model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=False))\n",
        "# model.add(Dropout(0.2))\n",
        "# #model.add(LSTM(256))\n",
        "# model.add(Dense(y.shape[1], activation='softmax'))\n",
        "# model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "adamOpt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adamOpt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po-uWv9Yvu24",
        "colab_type": "code",
        "outputId": "ffd1be8b-9d6e-4e0c-ba12-cd2dd5c320fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X, y, epochs=100, batch_size=32, callbacks=callbacks_list)"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "2716/2716 [==============================] - 21s 8ms/step - loss: 2.9740\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.97404, saving model to weights-improvement-01-2.9740.hdf5\n",
            "Epoch 2/100\n",
            "2716/2716 [==============================] - 14s 5ms/step - loss: 2.9136\n",
            "\n",
            "Epoch 00002: loss improved from 2.97404 to 2.91359, saving model to weights-improvement-02-2.9136.hdf5\n",
            "Epoch 3/100\n",
            "2716/2716 [==============================] - 14s 5ms/step - loss: 2.8995\n",
            "\n",
            "Epoch 00003: loss improved from 2.91359 to 2.89950, saving model to weights-improvement-03-2.8995.hdf5\n",
            "Epoch 4/100\n",
            "2716/2716 [==============================] - 14s 5ms/step - loss: 2.9022\n",
            "\n",
            "Epoch 00004: loss did not improve from 2.89950\n",
            "Epoch 5/100\n",
            "2716/2716 [==============================] - 14s 5ms/step - loss: 2.9024\n",
            "\n",
            "Epoch 00005: loss did not improve from 2.89950\n",
            "Epoch 6/100\n",
            "2716/2716 [==============================] - 14s 5ms/step - loss: 2.8994\n",
            "\n",
            "Epoch 00006: loss improved from 2.89950 to 2.89940, saving model to weights-improvement-06-2.8994.hdf5\n",
            "Epoch 7/100\n",
            "2716/2716 [==============================] - 14s 5ms/step - loss: 2.8943\n",
            "\n",
            "Epoch 00007: loss improved from 2.89940 to 2.89430, saving model to weights-improvement-07-2.8943.hdf5\n",
            "Epoch 8/100\n",
            "2716/2716 [==============================] - 14s 5ms/step - loss: 2.8939\n",
            "\n",
            "Epoch 00008: loss improved from 2.89430 to 2.89391, saving model to weights-improvement-08-2.8939.hdf5\n",
            "Epoch 9/100\n",
            "2716/2716 [==============================] - 14s 5ms/step - loss: 2.8899\n",
            "\n",
            "Epoch 00009: loss improved from 2.89391 to 2.88988, saving model to weights-improvement-09-2.8899.hdf5\n",
            "Epoch 10/100\n",
            "2716/2716 [==============================] - 14s 5ms/step - loss: 2.8949\n",
            "\n",
            "Epoch 00010: loss did not improve from 2.88988\n",
            "Epoch 11/100\n",
            "2716/2716 [==============================] - 14s 5ms/step - loss: 2.8904\n",
            "\n",
            "Epoch 00011: loss did not improve from 2.88988\n",
            "Epoch 12/100\n",
            "2716/2716 [==============================] - 14s 5ms/step - loss: 2.8921\n",
            "\n",
            "Epoch 00012: loss did not improve from 2.88988\n",
            "Epoch 13/100\n",
            "2716/2716 [==============================] - 14s 5ms/step - loss: 2.8909\n",
            "\n",
            "Epoch 00013: loss did not improve from 2.88988\n",
            "Epoch 14/100\n",
            "2716/2716 [==============================] - 14s 5ms/step - loss: 2.8870\n",
            "\n",
            "Epoch 00014: loss improved from 2.88988 to 2.88699, saving model to weights-improvement-14-2.8870.hdf5\n",
            "Epoch 15/100\n",
            "2716/2716 [==============================] - 14s 5ms/step - loss: 2.8873\n",
            "\n",
            "Epoch 00015: loss did not improve from 2.88699\n",
            "Epoch 16/100\n",
            "2716/2716 [==============================] - 14s 5ms/step - loss: 2.8881\n",
            "\n",
            "Epoch 00016: loss did not improve from 2.88699\n",
            "Epoch 17/100\n",
            "2716/2716 [==============================] - 14s 5ms/step - loss: 2.8886\n",
            "\n",
            "Epoch 00017: loss did not improve from 2.88699\n",
            "Epoch 18/100\n",
            "2716/2716 [==============================] - 14s 5ms/step - loss: 2.8853\n",
            "\n",
            "Epoch 00018: loss improved from 2.88699 to 2.88533, saving model to weights-improvement-18-2.8853.hdf5\n",
            "Epoch 19/100\n",
            "2716/2716 [==============================] - 14s 5ms/step - loss: 2.8826\n",
            "\n",
            "Epoch 00019: loss improved from 2.88533 to 2.88256, saving model to weights-improvement-19-2.8826.hdf5\n",
            "Epoch 20/100\n",
            "2716/2716 [==============================] - 14s 5ms/step - loss: 2.8865\n",
            "\n",
            "Epoch 00020: loss did not improve from 2.88256\n",
            "Epoch 21/100\n",
            "2716/2716 [==============================] - 14s 5ms/step - loss: 2.8809\n",
            "\n",
            "Epoch 00021: loss improved from 2.88256 to 2.88088, saving model to weights-improvement-21-2.8809.hdf5\n",
            "Epoch 22/100\n",
            "2716/2716 [==============================] - 14s 5ms/step - loss: 2.8827\n",
            "\n",
            "Epoch 00022: loss did not improve from 2.88088\n",
            "Epoch 23/100\n",
            "2716/2716 [==============================] - 14s 5ms/step - loss: 2.8793\n",
            "\n",
            "Epoch 00023: loss improved from 2.88088 to 2.87927, saving model to weights-improvement-23-2.8793.hdf5\n",
            "Epoch 24/100\n",
            "2716/2716 [==============================] - 14s 5ms/step - loss: 2.8758\n",
            "\n",
            "Epoch 00024: loss improved from 2.87927 to 2.87580, saving model to weights-improvement-24-2.8758.hdf5\n",
            "Epoch 25/100\n",
            "2716/2716 [==============================] - 14s 5ms/step - loss: 2.8757\n",
            "\n",
            "Epoch 00025: loss improved from 2.87580 to 2.87571, saving model to weights-improvement-25-2.8757.hdf5\n",
            "Epoch 26/100\n",
            "2716/2716 [==============================] - 14s 5ms/step - loss: 2.8794\n",
            "\n",
            "Epoch 00026: loss did not improve from 2.87571\n",
            "Epoch 27/100\n",
            "2716/2716 [==============================] - 14s 5ms/step - loss: 2.8782\n",
            "\n",
            "Epoch 00027: loss did not improve from 2.87571\n",
            "Epoch 28/100\n",
            "2716/2716 [==============================] - 14s 5ms/step - loss: 2.8763\n",
            "\n",
            "Epoch 00028: loss did not improve from 2.87571\n",
            "Epoch 29/100\n",
            "2716/2716 [==============================] - 14s 5ms/step - loss: 2.8727\n",
            "\n",
            "Epoch 00029: loss improved from 2.87571 to 2.87268, saving model to weights-improvement-29-2.8727.hdf5\n",
            "Epoch 30/100\n",
            "2716/2716 [==============================] - 14s 5ms/step - loss: 2.8712\n",
            "\n",
            "Epoch 00030: loss improved from 2.87268 to 2.87115, saving model to weights-improvement-30-2.8712.hdf5\n",
            "Epoch 31/100\n",
            "2716/2716 [==============================] - 14s 5ms/step - loss: 2.8707\n",
            "\n",
            "Epoch 00031: loss improved from 2.87115 to 2.87074, saving model to weights-improvement-31-2.8707.hdf5\n",
            "Epoch 32/100\n",
            "2716/2716 [==============================] - 14s 5ms/step - loss: 2.8679\n",
            "\n",
            "Epoch 00032: loss improved from 2.87074 to 2.86793, saving model to weights-improvement-32-2.8679.hdf5\n",
            "Epoch 33/100\n",
            "2716/2716 [==============================] - 14s 5ms/step - loss: 2.8651\n",
            "\n",
            "Epoch 00033: loss improved from 2.86793 to 2.86510, saving model to weights-improvement-33-2.8651.hdf5\n",
            "Epoch 34/100\n",
            "2716/2716 [==============================] - 14s 5ms/step - loss: 2.8635\n",
            "\n",
            "Epoch 00034: loss improved from 2.86510 to 2.86347, saving model to weights-improvement-34-2.8635.hdf5\n",
            "Epoch 35/100\n",
            "1088/2716 [===========>..................] - ETA: 8s - loss: 2.8428"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-139-a2b95e975a31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mt_before_callbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# will be handled by on_epoch_end.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0mevt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \"\"\"\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_waiters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0wl5jPAvu0v",
        "colab_type": "code",
        "outputId": "af7568e0-4b53-4d0d-d40c-21cfd7c1514f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "import sys\n",
        "filename = \"weights-improvement-56-2.3526.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = list(dataX[start])\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(500):\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = numpy.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\"  ladderwhy i hadnt to bring but one \"\n",
            "     ttttt      ttttt      tttt      tttt      ttttt      tttttt      ttttt      tttt      tttt      ttttt      tttttt      ttttt      tttt      tttt      ttttt      tttttt      ttttt      tttt      tttt      ttttt      tttttt      ttttt      tttt      tttt      ttttt      tttttt      ttttt      tttt      tttt      ttttt      tttttt      ttttt      tttt      tttt      ttttt      tttttt      ttttt      tttt      tttt      ttttt      tttttt      ttttt      tttt      tttt      ttttt      tttttt    \n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}